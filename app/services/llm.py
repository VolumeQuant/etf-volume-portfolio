import os, json, httpx

PROVIDER = os.getenv("PROVIDER", "").lower()
GROQ_API_KEY = os.getenv("GROQ_API_KEY")
GROQ_MODEL = os.getenv("GROQ_MODEL", "llama-3.1-8b-instant")

SYSTEM_PROMPT = (
    "ETF Í±∞ÎûòÎüâ Î∂ÑÏÑùÍ∞Ä. Îç∞Ïù¥ÌÑ∞Î•º Î≥¥Í≥† 3Î¨∏Ïû•ÏúºÎ°ú ÏöîÏïΩ: "
    "1) Ï£ºÎ™©Ìï† ETFÏôÄ Í±∞ÎûòÎüâ Ïä§ÌååÏù¥ÌÅ¨ ÎπÑÏú®, "
    "2) Í∞ÄÍ≤© Î∞òÏùë(ÏÉÅÏäπ/ÌïòÎùΩ), "
    "3) Ìà¨Ïûê ÏãúÏÇ¨Ï†ê."
)

def _rule_based_explain(user_content: str) -> str:
    try:
        data = json.loads(user_content)
    except Exception:
        return f"ÏûÖÎ†• Îç∞Ïù¥ÌÑ∞Î•º Ìï¥ÏÑùÌñàÏäµÎãàÎã§:\n{user_content[:200]}..."

    lines = []
    
    # Îπ†Î•∏ Ïä§Ï∫î Î™®Îìú Ï≤òÎ¶¨
    if data.get("mode") == "quick_scan":
        scan_data = data.get("data", [])
        timestamp = data.get("timestamp", "")
        
        lines.append(f"üìä Îπ†Î•∏ Ïä§Ï∫î Î∂ÑÏÑù (Í∏∞Ï§Ä: {timestamp[:19] if timestamp else 'Ïïå Ïàò ÏóÜÏùå'})")
        lines.append(f"Î™®ÎãàÌÑ∞ÎßÅ ETF Ïàò: {len(scan_data)}Í∞ú\n")
        
        # Í±∞ÎûòÎüâ Ïä§ÌååÏù¥ÌÅ¨ Í∏∞Ï§Ä Ï†ïÎ†¨
        sorted_data = sorted(scan_data, key=lambda x: x.get("volume_spike_ratio", 0) or 0, reverse=True)
        
        high_spikes = [d for d in sorted_data if (d.get("volume_spike_ratio") or 0) >= 1.5]
        
        if high_spikes:
            lines.append("‚ö° Ï£ºÎ™©Ìï† ETF (Í±∞ÎûòÎüâ Ïä§ÌååÏù¥ÌÅ¨ 1.5x Ïù¥ÏÉÅ):")
            for item in high_spikes[:5]:
                ticker = item.get("ticker", "?")
                name = item.get("name", "")
                spike = item.get("volume_spike_ratio", 0)
                price_chg = item.get("price_change_pct", 0)
                lines.append(f"  ‚Ä¢ {ticker} ({name}): Í±∞ÎûòÎüâ {spike:.2f}x, Í∞ÄÍ≤© {price_chg:+.2f}%")
            
            lines.append(f"\nüí° Í≤∞Î°†: {high_spikes[0].get('ticker')} Îì±ÏóêÏÑú Í±∞ÎûòÎüâ Í∏âÏ¶ù Í∞êÏßÄ. Îã®Í∏∞ Î™®Î©òÌÖÄ Ï£ºÎ™© ÌïÑÏöî.")
        else:
            lines.append("üí° ÌòÑÏû¨ ÎöúÎ†∑Ìïú Í±∞ÎûòÎüâ Ïù¥ÏÉÅÏßïÌõÑ ÏóÜÏùå. Í¥ÄÎßù Í∂åÏû•.")
        
        return "\n".join(lines)
    
    # Ï†ÑÏ≤¥ Î∂ÑÏÑù Î™®Îìú Ï≤òÎ¶¨
    metadata = data.get("metadata", {})
    summary = data.get("summary", {})
    top_spikes = data.get("top_spikes", [])
    
    lines.append(f"üìä Ï†ÑÏ≤¥ Î∂ÑÏÑù (Í∏∞Í∞Ñ: {metadata.get('date_range', {}).get('start', '?')} ~ {metadata.get('date_range', {}).get('end', '?')})")
    lines.append(f"Î∂ÑÏÑù ETF Ïàò: {metadata.get('tickers_analyzed', 0)}Í∞ú\n")
    
    # Ïù¥Î≤§Ìä∏ ÏöîÏïΩ
    total_events = summary.get("total_events", 0)
    by_level = summary.get("by_level", {})
    
    if total_events > 0:
        lines.append(f"üîç Í∞êÏßÄÎêú Í±∞ÎûòÎüâ Ïù¥Î≤§Ìä∏: {total_events}Í∞ú")
        if by_level:
            level_str = ", ".join([f"{level}: {count}Í∞ú" for level, count in by_level.items()])
            lines.append(f"  Î∂ÑÎ•ò: {level_str}\n")
        
        # ÏµúÎåÄ Ïä§ÌååÏù¥ÌÅ¨ ETF
        if top_spikes:
            lines.append("üî• ÏµúÎåÄ Í±∞ÎûòÎüâ Ïä§ÌååÏù¥ÌÅ¨ TOP 5:")
            for spike in top_spikes[:5]:
                ticker = spike.get("Ticker", "?")
                date = spike.get("Date", "")
                spike_ratio = spike.get("Volume_Spike_Ratio", 0)
                price_chg = spike.get("Price_Change_Pct", 0)
                lines.append(f"  ‚Ä¢ {ticker} ({date}): {spike_ratio:.2f}x Ïä§ÌååÏù¥ÌÅ¨, Í∞ÄÍ≤© {price_chg:+.2f}%")
            
            # ÏµúÍ∑º Ïù¥Î≤§Ìä∏ Î∂ÑÏÑù
            latest_events = summary.get("latest_events", [])
            if latest_events:
                recent_tickers = list(set([e.get("Ticker") for e in latest_events[:5]]))
                lines.append(f"\nüí° Í≤∞Î°†: ÏµúÍ∑º {', '.join(recent_tickers[:3])} Îì±ÏóêÏÑú Í±∞ÎûòÎüâ Ïù¥ÏÉÅÏßïÌõÑ Í∞êÏßÄ.")
                lines.append("   Ìè¨ÏßÄÏÖò ÏßÑÏûÖ Ïãú 2~3Ïùº Ï∂îÏÑ∏ ÏßÄÏÜç Ïó¨Î∂Ä ÌôïÏù∏ Í∂åÏû•.")
            else:
                lines.append(f"\nüí° Í≤∞Î°†: {top_spikes[0].get('Ticker')} Îì±Ïùò Í≥ºÍ±∞ Ïä§ÌååÏù¥ÌÅ¨ ÌôïÏù∏Îê®. ÌòÑÏû¨Îäî Í¥ÄÎßù Î™®Îìú Í∂åÏû•.")
        else:
            lines.append("\nüí° Í≤∞Î°†: Î∂ÑÏÑù Í∏∞Í∞Ñ ÎÇ¥ ÎöúÎ†∑Ìïú Í±∞ÎûòÎüâ Ïù¥ÏÉÅÏßïÌõÑ ÏóÜÏùå. Ï†ïÏÉÅ Î≤îÏúÑ ÎÇ¥ Í±∞Îûò.")
    else:
        lines.append("üîç Í∞êÏßÄÎêú Í±∞ÎûòÎüâ Ïù¥Î≤§Ìä∏ ÏóÜÏùå")
        lines.append("\nüí° Í≤∞Î°†: Î∂ÑÏÑù Í∏∞Í∞Ñ ÎÇ¥ ÌäπÏù¥ÏÇ¨Ìï≠ ÏóÜÏùå. Ï†ïÏÉÅÏ†ÅÏù∏ Í±∞ÎûòÎüâ ÏàòÏ§Ä Ïú†ÏßÄ.")
    
    return "\n".join(lines)

async def _explain_with_groq(user_content: str) -> str:
    if not GROQ_API_KEY:
        raise RuntimeError("GROQ_API_KEY ÏóÜÏùå")
    if not user_content or not user_content.strip():
        raise RuntimeError("ÏûÖÎ†• Îç∞Ïù¥ÌÑ∞Í∞Ä ÎπÑÏóàÏäµÎãàÎã§")

    url = "https://api.groq.com/openai/v1/chat/completions"
    headers = {
        "Authorization": f"Bearer {GROQ_API_KEY}",
        "Content-Type": "application/json",
    }
    messages = [
        {"role": "system", "content": SYSTEM_PROMPT},
        {"role": "user", "content": user_content}
    ]
    payload = {
        "model": GROQ_MODEL,
        "messages": messages,
        "temperature": 0.4,
        "max_tokens": 200,  # ÌÜ†ÌÅ∞ Ï†àÏïΩ
    }
    async with httpx.AsyncClient(timeout=60) as client:
        r = await client.post(url, headers=headers, json=payload)
        # 400Ïùº Îïå ÏÑúÎ≤ÑÍ∞Ä Ï£ºÎäî ÏóêÎü¨ Î≥∏Î¨∏ÏùÑ Í∑∏ÎåÄÎ°ú ÎÖ∏Ï∂úÌï¥ ÏõêÏù∏ ÌååÏïÖ
        if r.status_code >= 400:
            try:
                err = r.json()
            except Exception:
                err = {"error_text": r.text}
            raise RuntimeError(f"Groq API {r.status_code}: {err}")
        data = r.json()
        return data["choices"][0]["message"]["content"]

async def explain(user_content: str) -> str:
    if PROVIDER == "groq":
        try:
            return await _explain_with_groq(user_content)
        except Exception as e:
            return _rule_based_explain(user_content) + f"\n\n[Ï∞∏Í≥†] Groq Ìè¥Î∞±: {e}"
    return _rule_based_explain(user_content)
